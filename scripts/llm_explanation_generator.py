# =============================================================================
# LLM Explanation Generator for Cancer Detection XAI
# Purpose: Generate layman-friendly explanations using LLaMA 3.2 for XAI outputs
# Environment: Python with requests library
# Date: October 2023
# =============================================================================

# Import Libraries
import json
import requests
import time

# =============================================================================
# Configuration
# Set the path to the XAI output file generated by the test script
# =============================================================================

XAI_OUTPUT_FILE = "outputs/xai_output.json"  # Path to the JSON file from test_with_h5.py

# Hugging Face Inference API endpoint for LLaMA 3.2-3B-Instruct (free tier, requires API token for stability)
API_URL = "https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct"

# Replace 'YOUR_HUGGING_FACE_API_TOKEN' with your actual token from huggingface.co
HUGGING_FACE_API_TOKEN = "insert-your-token-here"  # Your token from Hugging Face

# =============================================================================
# Functions
# Handle LLM API calls and explanation generation
# =============================================================================

def get_llm_explanation(pred_class, class_name, xai_insights, confidence):
    """
    Use Hugging Face's Inference API to get a layman-friendly explanation from LLaMA 3.2-3B-Instruct.
    
    Args:
        pred_class (int): Predicted class index.
        class_name (str): Name of the predicted class (e.g., "Brain Cancer" or "ALL").
        xai_insights (str): Summary of XAI findings.
        confidence (float): Model confidence in percentage.
    
    Returns:
        str: Layman-friendly explanation.
    """
    parts = class_name.split()
    if len(parts) > 1:
        location = parts[0].lower()
        tumor_type = parts[1].lower()
    else:
        location = "the affected area"
        tumor_type = class_name.lower()

    tumor_label = f"{tumor_type} tumor" if tumor_type not in ["all"] else f"{tumor_type} condition"

    tumor_location = "in the middle of the body"
    if "brain" in class_name.lower():
        tumor_location = "in the center of the brain" if "brain center" in xai_insights.lower() else "in the brain"
    elif "breast" in class_name.lower():
        tumor_location = "in the breast tissue"
    elif "lung" in class_name.lower():
        tumor_location = "in the lung tissue"
    elif "colon" in class_name.lower():
        tumor_location = "in the colon tissue"
    elif "cervical" in class_name.lower():
        tumor_location = "in the cervical tissue"
    elif "kidney" in class_name.lower():
        tumor_location = "in the kidney tissue"
    elif "oral" in class_name.lower():
        tumor_location = "in the oral cavity"
    elif "lymphoma" in class_name.lower():
        tumor_location = "in the lymphatic system"

    medical_context = f"itâ€™s a common {tumor_type} {('tumor' if tumor_type not in ['all'] else 'condition')}" if tumor_type not in ["all"] else f"itâ€™s a type of blood cancer involving abnormal lymphocytes"

    prompt = (f"Explain in simple, friendly, conversational terms why an AI predicted {class_name} (specifically {location} "
              f"{tumor_label}) for a patientâ€™s medical image "
              f"with {confidence:.1f}% confidence. Mention the tumorâ€™s location ({tumor_location}, "
              f"based on the medical context). Include basic medical context ({medical_context}) "
              f"and keep it short, clear, and positive for non-experts, with an optional emoji for reassurance like ðŸ˜Š. "
              f"Avoid adding fictional names, tools, studies, or unnecessary details. "
              f"For doctors, add a concise technical note in parentheses about the XAI focus (Grad-CAM, Saliency Map, LIME).")

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {HUGGING_FACE_API_TOKEN}"
    }
    
    data = {"inputs": prompt, "parameters": {"max_length": 150, "temperature": 0.7, "top_p": 0.9}}
    
    try:
        response = requests.post(API_URL, headers=headers, json=data, timeout=10)
        response.raise_for_status()
        print(f"Successfully used {API_URL}")
        return response.json()[0]["generated_text"]
    except requests.exceptions.RequestException as e:
        print(f"Error calling Hugging Face API ({API_URL}): {e}")
        time.sleep(30)
        try:
            response = requests.post(API_URL, headers=headers, json=data, timeout=10)
            response.raise_for_status()
            print(f"Retry successful for {API_URL}")
            return response.json()[0]["generated_text"]
        except requests.exceptions.RequestException as e2:
            print(f"Retry failed: {e2}")
            headers = {"Content-Type": "application/json"}
            try:
                response = requests.post(API_URL, headers=headers, json=data, timeout=10)
                response.raise_for_status()
                print(f"Successfully used {API_URL} unauthenticated")
                return response.json()[0]["generated_text"]
            except requests.exceptions.RequestException as e3:
                print(f"Error with unauthenticated request: {e3}")
                return "Sorry, I couldnâ€™t generate an explanation right now."

def load_xai_data(file_path):
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
            print("Loaded XAI data:", data)
            return data
    except FileNotFoundError:
        print(f"Error: Could not find {file_path}. Ensure the test script ran and saved the file.")
        return None
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON in {file_path}. Check the file format.")
        return None

# =============================================================================
# Main Execution
# Load XAI data list and generate explanations for each entry
# =============================================================================

def main():
    print("Starting LLM Explanation Generator...")
    
    try:
        with open(XAI_OUTPUT_FILE, 'r') as f:
            xai_data_list = json.load(f)
            if not isinstance(xai_data_list, list):
                xai_data_list = [xai_data_list]
    except FileNotFoundError:
        print(f"Error: Could not find {XAI_OUTPUT_FILE}. Ensure the test script ran and saved the file.")
        return
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON in {XAI_OUTPUT_FILE}. Check the file format.")
        return
    
    for xai_data in xai_data_list:
        pred_class = xai_data["predicted_class"]
        class_name = xai_data["class_name"]
        confidence = xai_data["confidence"]
        xai_insights = xai_data["xai_insights"]
        
        explanation = get_llm_explanation(pred_class, class_name, xai_insights, confidence)
        print(f"\nExplanation for Image {xai_data.get('image_index', 'Unknown')}:")
        print("Laymanâ€™s Explanation:", explanation)

if __name__ == "__main__":
    main()
# =============================================================================